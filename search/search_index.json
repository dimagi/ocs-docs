{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Open Chat Studio","text":"<p>This is the home page for all documentation related to Open Chat Studio. Developed by Dimagi, Open Chat Studio is an easy-to-use, open source platform for rapidly prototyping, testing and deploying chatbots created using Large Language Models (LLMs).</p>"},{"location":"#what-can-i-do-on-open-chat-studio","title":"What can I do on Open Chat Studio?","text":"<ul> <li> <p>Make Your Own Chatbots: With Open Chat Studio (OCS), you can easily create your own chatbots using advanced   language technology. OCS is built for use by program staff and other teams - you don't need to be an engineer to get   started.</p> </li> <li> <p>Deploy: Use Open Chat Studio to launch your chatbots on the web and mobile apps such as Telegram and WhatsApp,   with more   options coming soon.</p> </li> <li> <p>Enable Access for All: Anyone you share a chatbot with will be able to access it, either through a web link or   directly   on platforms such as WhatsApp or Telegram. Chatbot users do not need to have an account on Open Chat Studio to use   your   chatbots.</p> </li> <li> <p>View and Download Data: View and export the data from interactions with your chatbots, formatted In CSV.</p> </li> </ul>"},{"location":"#how-do-i-use-open-chat-studio","title":"How do I use Open Chat Studio?","text":"<p>You can host your own instance of Open Chat Studio, or use the hosted version at chatbots.dimagi.com.</p> <p>If you would like an account on Dimagi's hosted version of Open Chat Studio send an email to ocs-info@dimagi.com. </p>"},{"location":"about/","title":"About Open Chat Studio","text":"<p>Dimagi is developing Open Chat Studio (OCS) as an easy-to-use, open source platform for rapidly prototyping and testing chatbots created using Large Language Models (LLMs). Open Chat Studio makes it easy to develop and test LLM-based chatbots, and to instill a variety of guardrails to improve the safety and accuracy of these bots.</p> <p>Open Chat Studio can work with any LLM with an API such as the OpenAI Chat Completions API.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#feb-21-2025","title":"Feb 21, 2025","text":"<ul> <li>NEW: Add an AI helper for writing code in Python Nodes. Demo</li> </ul>"},{"location":"changelog/#feb-19-2025","title":"Feb 19, 2025","text":"<ul> <li>NEW: Add support Deepseek models.</li> <li>NEW: Added theme toggle in the navbar which allows switching between dark and light theme. </li> <li>CHANGE The experiment description can now include markdown formatting which will be rendered when it is displayed on the public chatbot pages (the consent page and the pre-survey page).</li> <li>CHANGE All markdown links in the chatbot now open in a new tab.</li> </ul>"},{"location":"changelog/#feb-18-2025","title":"Feb 18, 2025:","text":"<ul> <li>CHANGE Pipelines are now sorted by name in ascending order in both the pipelines table and dropdown list on the experiment edit page.</li> <li>CHANGE Allow unauthenticated users (public chat users) to access attachments in their chats.</li> </ul>"},{"location":"changelog/#feb-14-2025","title":"Feb 14, 2025:","text":"<ul> <li>BUG Fixed an issue where the citations enabled toggle on an assistant node showed that it was disabled when the node was just added, where in reality it was actually enabled.</li> <li>CHANGE Update the embeddable chat widget to better support mobile devices &amp; allow more customization of the styles. </li> <li>CHANGE Allow pipeline bots to toggle conversational consent.</li> </ul>"},{"location":"changelog/#feb-06-2025","title":"Feb 06, 2025:","text":"<ul> <li>NEW Chatbots can now be embedded into external websites. See the documentation for more information.</li> <li>CHANGE Enhanced the participants filter in the experiment sessions view to support multiple participants. This allows you to export chats for selected participants only.</li> <li>CHANGE Consent forms have become optional. If no consent form is configured then the user will not be prompted to accept a consent form before starting a chat. Pre- and post-surveys are still displayed if there are any configured.</li> </ul>"},{"location":"changelog/#jan-29-2025","title":"Jan 29, 2025:","text":"<ul> <li>CHANGE Improved pipeline validation logic and display of errors.</li> <li>CHANGE Improve the changes UI when creating a new experiment version to show the details of referenced objects (e.g. pipelines, assistants, etc).</li> <li> <p>CHANGE Tag UI improvements to make it easier to add and remove tags from chats and messages.</p> </li> <li> <p>NEW Chatbot versioning released. See the documentation for more information.</p> </li> <li>NEW New nodes added to Pipelines.<ul> <li>Python Node: Allows you to run Python code in the pipeline.</li> <li>OpenAI Assistant Node: Allows you to use an OpenAI Assistant in the pipeline.</li> <li>Static Router Node: Allows you to route messages to different nodes based on a static mapping.</li> </ul> </li> <li>NEW Pipelines can now be tested via the edit UI. This does not currently support history or participant data.</li> <li>NEW \"Trigger Bot Message\" API endpoint. This allows you to send a prompt to a bot which will trigger a message to the specified user. See the API documentation for more information.</li> <li>NEW Custom Actions allow bots to connect to external APIs and services.</li> <li>NEW The web chat UI now supports  and  reactions to bot messages. These reactions are saved as tags on the message.</li> </ul>"},{"location":"changelog/#dec-12-2024","title":"Dec 12, 2024:","text":"<ul> <li>NEW When an experiment is in \u201cdebug mode\u201d, errors will be shown to the user in the chat UI.</li> </ul>"},{"location":"changelog/#nov-6-2024","title":"Nov 6, 2024:","text":"<ul> <li> <p>CHANGE Auto populate channel names with the experiment\u2019s name and improve the channel help text</p> </li> <li> <p>NEW Added the ability to toggle whether responses should include source citations in the case of assistant based   experiments. To toggle this, you\u2019ll need to have the \u201cassistant\u201d type selected in the experiment\u2019s edit screen, then   go   to the \u201cAdvanced\u201d tab. There you will see a \u201cCitations enabled\u201d setting. When this setting is \u201coff\u201d, no citations   will   be present in the response.</p> </li> </ul>"},{"location":"changelog/#oct-29-2024","title":"Oct 29, 2024:","text":"<ul> <li> <p>CHANGE [Security update] Participants will now have to verify their email address before they can continue to   the chat   session.</p> </li> <li> <p>NEW Added a new \u201cdebug mode\u201d for experiments. When an experiment is in debug mode, the user will be able to see.</p> </li> </ul>"},{"location":"changelog/#aug-16-2024","title":"Aug 16, 2024:","text":"<ul> <li>CHANGE API update: Ability to filter experiment session results using a newly supported \u201ctags\u201d parameter.</li> </ul>"},{"location":"changelog/#aug-16-2024_1","title":"Aug 16, 2024:","text":"<ul> <li>NEW Integration with Langfuse for tracing.</li> <li>NEW Dynamic voice support in a multi bot setup. Users can now choose to let the child bot\u2019s voice be used if it   generated the output in a multi-bot setup. This behaviour is disabled by default, but can be enabled by going to the   experiment\u2019s voice configuration settings and checking the Use processor bot voice box.</li> </ul>"},{"location":"changelog/#aug-14-2024","title":"Aug 14, 2024:","text":"<ul> <li>BUG Fixed tagging in the case where assistants were used as child bots in a router setup</li> <li>CHANGE Assistants cannot be used as router bots anymore, since this messes up the conversation history on OpenAI\u2019s   side.</li> </ul>"},{"location":"changelog/#aug-5-2024","title":"Aug 5, 2024:","text":"<ul> <li>NEW Allow sorting of the \u201cStarted\u201c and \u201cLast Message\u201c columns in the experiment sessions view</li> <li>NEW Terminal bot. You can now configure a bot (from an experiment\u2019s homepage) to run at the end of every inference   call. This terminal bot will change the input according to the configured prompt, even in a multi-bot configuration. A   terminal bot is useful when you want to ensure that the bot always responds in a certain language.</li> <li>CHANGE The {source_material} and {participant_data} prompt variables can now only be used once in a prompt.   Including this variable more than once in the same prompt will not be allowed.</li> <li>BUG Fixed an issue where assistant generated Word (.docx) files (and possibly others) were being corrupted.</li> </ul>"},{"location":"changelog/#aug-1-2024","title":"Aug 1, 2024:","text":"<ul> <li>CHANGE Improved data extraction to handle long contexts</li> </ul>"},{"location":"changelog/#july-26-2024","title":"July 26, 2024:","text":"<ul> <li>NEW File download support for assistant bots. Cited files that were uploaded by the user or generated by the   assistant   can now be downloaded from within the chat. Please note that this only applies to webchats, and the user must be a   logged in user to download these files.</li> <li>NEW Twilio numbers will now be verified at the selected provider account before allowing the user to link the   whatsapp account to an experiement. Please note that this will not be done for Turn.io numbers, since they do not   provide a mechanism for checking numbers.</li> </ul>"},{"location":"changelog/#july-19-2024","title":"July 19, 2024:","text":"<ul> <li>NEW In-conversation file uploads for assistant bots on web based chats. These files will be scoped only to the   current   chat/OpenAI thread, so other sessions with the same bot will not be able to query these files.</li> </ul>"},{"location":"changelog/#july-15-2024","title":"July 15, 2024:","text":"<ul> <li>NEW Participant data extraction through pipelines.   You need the \u201cPipelines\u201d feature flag enabled.   Usage information can be found here.</li> <li>BUG Normalize numbers when adding or updating a whatsapp channel. This helps to avoid accidentally creating   another whatsapp channel with the same number that is in a different format.</li> <li>BUG Verify Telegram token when adding a telegram channel</li> </ul>"},{"location":"changelog/#july-8-2024","title":"July 8, 2024:","text":"<ul> <li>NEW Add {current_datetime} prompt variable to inject the current date and time into the prompt</li> <li>BUG Fixed a bug with syncing files to OpenAI assistants vector stores</li> <li>BUG Ensure API keys have the current team attached to them</li> <li>BUG Enforce team slugs to be lowercase</li> <li>BUG Update chat history compression to take the full prompt into account including source material, participant   data   etc.</li> <li>CHANGE Redo UI to show team on all pages and use dropdown for team switching and team management links</li> <li>CHANGE Hide API sessions from \u2018My Sessions\u2019 list</li> <li>NEW User interface for creating and editing Experiment Routes (parent / child experiments)   New tab on the main experiment page</li> </ul> <p>API changes</p> <ul> <li>NEW API documentation and schema now available at https://chatbots.dimagi.com/api/docs/</li> <li>NEW Experiment session list, detail and create API</li> <li>CHANGE Channel message API now takes an optional session field in the request body to specify the ID of a specific     session</li> <li>CHANGE Experiment API output renamed experiment_id to id</li> <li> <p>CHANGE Update participant data API POST body format changed. New format:</p> <p><code>[{\"experiment\": \"&lt;experiment_id&gt;\", \"data\": {\"property1\": \"value1\"}}]</code></p> </li> <li> <p>NEW OpenAI compatible \u2018Chat Completions\u2019 API for experiments. See API docs.</p> </li> </ul>"},{"location":"changelog/#jun-24-2024","title":"Jun 24, 2024","text":"<ul> <li>NEW Tagging messages based on which bot generated that response in a multi-bot setup</li> </ul>"},{"location":"changelog/#jun-17-2024","title":"Jun 17, 2024","text":"<ul> <li>NEW Pipelines (alpha)<ul> <li>Look for \"Pipelines\" in the sidebar</li> <li>Ability to run a pipeline based on event triggers</li> <li>Ability to create a pipeline visually</li> </ul> </li> <li>NEW Participant view<ul> <li>View participants with their data</li> <li>We added a view where users can see all participants, when they joined initially and what experiments they participated in.</li> <li>Experiment admins will also be able to add or update data to be associated with a participant. When the experiment prompt includes the <code>{participant_data}</code> variable, this data will be visible to the bot and participant specific details can be referenced or considered during the conversation.</li> <li>Note that participant data is scoped to the specific experiment. This means that you have to manually add data pertaining to a particular participant to each experiment manually.</li> </ul> </li> <li>NEW Slack integration<ul> <li>Ability to connect to a slack workspace as a messaging provider</li> <li>Ability to link an experiment to a Slack Channel (via an experiment channel)</li> <li>Ability to have one 'fallback' experiment which can respond to messages on channels where no other experiment is assigned</li> <li>Experiment responds to 'mentions' and creates a new session as a Slack thread</li> </ul> </li> </ul>"},{"location":"changelog/#jun-4-2024","title":"Jun 4, 2024","text":"<ul> <li>CHANGE Individual tool selection<ul> <li>Users can now choose which tools to give to the bot to use.</li> <li>Previously this was obscured by a single \u201ctools enabled\u201d checkbox which - when enabled - gave the bot all the tools that existed at that time.</li> <li>Tools include: One-off Reminder, Recurring Reminder and Schedule Update</li> <li>One-off Reminder: This allows the bot to create a one-time reminder message for some time in the future.</li> <li>Recurring Reminder: This allows the bot to create recurring reminder messages.</li> <li>Schedule Update: This allows the bot to update existing scheduled messages. Please note that this tool cannot update reminders created with the one-off and recurring reminder tools, since those are legacy tools using a different framework. Future work will fix this.</li> </ul> </li> <li>CHANGE When and error occurs during the processing of a user message, the bot will tell the user that something went     wrong instead of keeping quiet</li> </ul>"},{"location":"changelog/#may-21-2024","title":"May 21, 2024","text":"<ul> <li>NEW OpenAI Assistants v2 support</li> <li>NEW Multi-experiment bots<ul> <li>Allows users to combine multiple bots into a single bot, called the \u201cparent\u201d bot. The parent bot will decide which of the \u201cchild\u201d bots the user query should be routed to, based on the prompt.</li> <li>The prompt should give clear instruction to the parent bot on how it should decide to route the user query.</li> </ul> </li> </ul>"},{"location":"changelog/#may-15-2024","title":"May 15, 2024","text":"<ul> <li>CHANGE Experiment search improvements<ul> <li>The search string will be used to search for experiments by name first, then by description </li> </ul> </li> <li>NEW OpenChatStudio API: A few endpoints are exposed to allow chatting to a bot using the API</li> </ul>"},{"location":"changelog/#may-10-2024","title":"May 10, 2024","text":"<ul> <li>NEW Bots are given knowledge of the date and time</li> <li>NEW Added a new event type called \u201cA new participant joined the experiment\u201d</li> <li>NEW Added a new event handler to create scheduled messages</li> </ul>"},{"location":"changelog/#may-9-2024","title":"May 9, 2024","text":"<ul> <li>NEW Show participant data in the session review page<ul> <li>If participant data exists and was included in the prompt, then reviewers will be able to see the data in the session review page. Seeing the data that the bot had access to might help with understanding the conversation.</li> </ul> </li> <li>CHANGE Anthropic now also supports tool usage!</li> </ul>"},{"location":"changelog/#apr-30-2024","title":"Apr 30, 2024","text":"<ul> <li>NEW Participant data<ul> <li>Participant data allows the bot to tailor responses by considering details about the participant.</li> <li>To include participant data in the prompt, go to the experiment edit page and add the <code>{participant_data}</code> variable to an appropriate place in the prompt.</li> <li>Currently we have to create it manually through the admin dashboard, but a near future release will include a dedicated page to view/edit participant data.</li> <li>Please note that participant data is experiment specific, meaning that data we have for a participant in one experiment may not be the same for the next experiment.</li> </ul> </li> </ul>"},{"location":"concepts/","title":"Conceptual Guide","text":"<p>This guide provides explanations of the key concepts behind the Open Chat Studio platform and AI applications more broadly.</p> <p>The conceptual guide does not cover step-by-step instructions or specific examples \u2014 those are found in the How-to guides.</p>"},{"location":"concepts/#terms","title":"Terms","text":"Assistant A chatbot that uses OpenAI`s Assistant API. Assistants can write and execute code and search and reference   information in uploaded files. Authentication Provider Authentication providers allow you to authenticate with external systems to access data or services. Channel The platform through which a chat occurs (e.g., WhatsApp, Telegram, Web, Slack). Consent Forms Forms that provide context to chatbot users on how their data will be used and who to contact regarding any concerns. Custom Actions Custom actions are a way to extend the functionality of the bot by integrating with external systems via HTTP APIs. Events Events are a way to trigger actions in the bot based on specific conditions. Experiment The current name used in Open Chat Studio to refer to a chatbot. An experiment links all the configuration and data for a chatbot, including user sessions, data, actions, etc. Large Language Models (LLMs) Large language models are a type of AI model that can generate human-like text, images and audio. Messaging Provider Messaging providers hold the configuration required to send messages to users on a specific channel. Participant Data Data that persists across sessions and is tied to the same <code>User, Channel, Chatbot</code> scope. It helps retain long-term user preferences and contextual information beyond a single session. Pipelines A pipeline is a way to build a bot by combining one or more steps together. Prompt A prompt is the instructions that are given to the LLM to generate a response. Prompts can include text, source material, and other variables. Session The scope of conversations between a user and a chatbot within a specific channel. Sessions are isolated, ensuring data privacy and contextual continuity for the duration of an interaction. Source Material Additional information that can be included in the bot prompt using the <code>{source_material}</code> prompt variable. Versions The ability to create and manage different versions of a chatbot."},{"location":"concepts/assistants/","title":"OpenAI Assistants","text":""},{"location":"concepts/assistants/#syncing-with-openai","title":"Syncing with OpenAI","text":"<p>The in-sync status with OpenAI is automatically checked each time a user visits the edit screen of an assistant. If the assistant in OCS has the identical configurations and files with the assistant in Open AI, the an in-sync status will appear under the assistant id: </p> <p>Otherwise, a warning will be displayed explaining what is out of sync. For example, in the image below, there are files uploaded in OpenAI that are not uploaded in OCS. This may result in unexpected behavior from the assistant. To resolve, upload the listed files in the edit screen. </p> <p>Assistants in versioned experiments</p> <p>Although an assistant cannot be modified in OCS once an experiment is released that references that assistant, it can still be modified in OpenAI. A new assistant in OpenAI will be created at the time of the experiment's release, and it is recommended not to modify that assistant to maintain the expected functionality of the released experiment.</p>"},{"location":"concepts/assistants/#archiving","title":"Archiving","text":"<ul> <li>Goal: Archiving an assistant in OCS deletes the associated assistant in OpenAI. This is an easy way to stop incurring costs and ensure that the assistant is closed for all experiments and pipelines that reference it within a project.</li> <li>OCS first checks if any published or unreleased experiments and pipelines reference the assistant. If so, the archival process is blocked, and a modal appears listing those experiments and pipelines preventing the archival. If only released versions reference the assistant, they are archived automatically.</li> <li>If any published or unreleased experiments reference the assistant, they are listed under the \u201cExperiments\u201d section of the modal. For published experiments, navigate to those versions and archive them. For unreleased versions, you must navigate to the version and either (1) remove the assistant reference or (2) archive the experiment.</li> <li>If a pipeline that references an assistant is not referenced by an experiment, the pipeline must be archived. These pipelines are listed under the \u201cPipelines\u201d section of the modal. Navigate to the pipeline and either archive it or remove the assistant reference to unblock the assistant archival.</li> <li>If a pipeline references an assistant and that pipeline is used by a published experiment, you must archive the experiment. These experiments are listed under the \u201cExperiments Referencing Pipeline\u201d section of the modal. The links direct you to the experiments where the pipeline is used. Navigate to the version listed in the modal and archive it.</li> </ul> <p>Archiving an assistant with versions</p> <p>If the assistant you are trying to archive has versions, the same checks apply to all versions of the assistant and are displayed together in the modal. Once confirmed, all assistant versions will be archived.</p>"},{"location":"concepts/authentication-providers/","title":"Authentication Providers","text":"<p>Authentication Providers are used to authenticate with external services via HTTP API calls. Authentication Providers provide a centralized location to manage the credentials and tokens required to authenticate with external services.</p> <p>These credentials are used by features like Custom Actions.</p>"},{"location":"concepts/authentication-providers/#authentication-provider-types","title":"Authentication Provider Types","text":"<p>Open Chat Studio supports various different authentication types. You should select the type that matches the API service you will be using.</p>"},{"location":"concepts/authentication-providers/#basic-auth","title":"Basic Auth","text":"<p>Basic Auth is a simple authentication scheme built into the HTTP protocol.</p>"},{"location":"concepts/authentication-providers/#api-key","title":"API Key","text":"<p>API Key is a simple authentication scheme that involves sending a key with the request to authenticate the user. The key is sent in a header of the request. The name of the header can be customized when creating the Authentication Provider.</p>"},{"location":"concepts/authentication-providers/#bearer-token","title":"Bearer Token","text":"<p>Bearer Token is a type of access token that is sent with the request to authenticate the user. The token is sent in the Authorization header of the request.</p>"},{"location":"concepts/authentication-providers/#commcare","title":"CommCare","text":"<p>CommCare HQ uses a custom authentication scheme as described in the CommCare Documentation</p>"},{"location":"concepts/channels/","title":"Channels","text":"<p>To enable users to interact with your bot through external social media platforms and similar services, OCS integrates with various messaging providers. This integration allows you to deploy your bot to external platforms. Once a platform is linked to your bot, users can communicate with it through that platform. In OCS, the term \"channel\" is synonymous with \"platform.\"</p> <p>The currently supported channels are:</p> <ul> <li>Telegram</li> <li>WhatsApp</li> <li>Facebook Messenger</li> <li>Slack</li> <li>API</li> <li>SureAdhere</li> </ul>"},{"location":"concepts/channels/#see-also","title":"See also","text":"<ul> <li>Deploying your bot to different channels</li> </ul>"},{"location":"concepts/consent/","title":"Consent Forms","text":"<p>Consent forms allow chatbot makers to provide context to chatbot users on how their data will be used, and who to contact regarding any concerns. Consent forms are displayed to users before they start interacting with the chatbot.</p> <p></p> <p>Using consent forms on WhatsApp, Telegram and other channels</p> <p>If you are deploying your chatbot on WhatsApp, Telegram or any channels other than the Web channel, you can still include consent forms in your chatbot by enabling the 'Conversational Consent' option for the chatbot.</p> <p>A default consent form is created for each team. You can customize this default form or create new forms by navigating to the \"Consent Forms\" section of the platform.</p>"},{"location":"concepts/consent/#what-to-put-in-a-consent-form","title":"What to put in a consent form","text":"<p>Some common elements you may want to include in a consent form are:</p> <ul> <li>A disclaimer stating that the accuracy of chatbot responses is not guaranteed.</li> <li>How you might use data from chatbot interactions.</li> <li>An email address and phone number for the relevant team responsible for managing the chatbot.</li> </ul>"},{"location":"concepts/custom_actions/","title":"Custom Actions","text":"<p>Custom Actions enable bots to communicate with external services via HTTP API calls.  This feature allows you to extend the functionality of your bot by integrating it with other services.</p> <p>This feature is analogous to the OpenAI's GPT Actions feature.</p>"},{"location":"concepts/custom_actions/#custom-action-fields","title":"Custom Action Fields","text":""},{"location":"concepts/custom_actions/#authentication-provider","title":"Authentication Provider","text":"<p>Before you create a Custom Action will need to create an Authentication Provider for your action to use (unless the API you are using does not require authentication). You can do this by navigating to the Authentication Providers section in Team Settings and creating a new Authentication Provider.</p>"},{"location":"concepts/custom_actions/#base-url","title":"Base URL","text":"<p>This is the URL of the external service you want to communicate with. For example: <code>https://www.example.com</code>. Only HTTPS URLs are supported.</p>"},{"location":"concepts/custom_actions/#api-schema","title":"API Schema","text":"<p>This is a JSON or YAML OpenAPI Schema document.</p> <p>You should be able to get this from the service you want to connect to. For example, the default location for the schema for FastAPI services is <code>/openapi.json</code> (https://fastapi.tiangolo.com/tutorial/first-steps/#openapi-and-json-schema).</p>"},{"location":"concepts/custom_actions/#how-custom-actions-work","title":"How Custom Actions work","text":"<p>When you create a custom action, each API endpoint in the OpenAPI schema will be available as a separate action in the Experiment configuration. This gives you full control over which actions are available to your bot.</p> <p>When you add a Custom Action to your Experiment, the bot will be able to make HTTP requests to the external service using the API endpoints you have configured. The bot will send the request and receive the response from the external service, which it can then use to generate a response to the user.</p>"},{"location":"concepts/events/","title":"Events","text":"<p>Open Chat Studio provides an event system that allows you to define actions triggered by specific events within a chat session. This functionality enables you to automate responses, manage session states, and enhance user interactions effectively.</p>"},{"location":"concepts/events/#overview","title":"Overview","text":"<p>Events in Open Chat Studio are categorized into two types:</p> <ol> <li>Static Events: Triggered by specific actions or occurrences within the chat session.</li> <li>Timeout Events: Triggered after a specified duration of inactivity following the last interaction.</li> </ol> <p>Each event has one action associated with it that is executed when the event occurs.</p>"},{"location":"concepts/events/#static-events","title":"Static Events","text":"<p>Static events are predefined triggers that occur based on specific actions or conditions within the chat session. The available static events are:</p> <ul> <li>Conversation End: Triggered when the conversation ends.</li> <li>Last Timeout: Triggered when the last timeout of any configured timeout events occur.</li> <li>Human Safety Layer Triggered: Triggered when the safety layer is activated by a message from the user.</li> <li>Bot Safety Layer Triggered: Triggered when the safety layer is activated by a response from the bot.</li> <li>Conversation Start: Triggered when a new conversation is started.</li> <li>New Human Message: Triggered when a new human message is received.</li> <li>New Bot Message: Triggered when a new bot message is received.</li> <li>Participant Joined Experiment: Triggered when a participant starts interacting with the bot for the very first time.</li> </ul>"},{"location":"concepts/events/#event-actions","title":"Event Actions","text":"<p>Each event is associated with one action. The available actions are:</p> <ul> <li>End the conversation: Ends the conversation with the user. See Resetting Sessions.</li> <li>Prompt the bot to message the user: Prompts the bot to message the user.</li> <li>Trigger a schedule: This will create a once off or recurring schedule. Each time the schedule is triggered, the bot will be prompted to message the user.</li> <li>Start a pipeline: This will run the given pipeline when the event triggers. The input to the pipeline can be configured.</li> </ul>"},{"location":"concepts/llm/","title":"Large Language Models (LLMs)","text":"<p>Definition</p> <p>A Large Language Model (or LLM) is a type of artificial intelligence software that is trained on a vast amount of text data. Its primary function is to understand, interpret, and generate human language. This training allows it to produce text-based responses, answer questions, translate between languages, and perform various other language-related tasks. </p> <p>The term \"large\" in its name refers to the extensive volume of data it has been trained on and the complexity of its design, enabling it to handle complex language tasks.</p> <p>The definition above was authored by the famous LLM that powers ChatGPT: GPT-4 developed by OpenAI.</p> <p>When building chatbots, an LLM powers the chatbot's ability to understand and respond to user inputs, effectively acting as the brain behind the chatbot.</p>"},{"location":"concepts/llm/#which-large-language-models-are-supported-by-open-chat-studio","title":"Which Large Language Models are supported by Open Chat Studio?","text":"<p>Open Chat Studio is developed to support a range of LLMs. The platform is designed to be flexible and can work with any LLM that has an API. The platform currently supports all the models provided by the following APIs:</p> <ul> <li>OpenAI</li> <li>Azure OpenAi</li> <li>Anthropic</li> <li>Groq</li> <li>Perplexity</li> <li>Deepseek</li> </ul>"},{"location":"concepts/llm/#model-configuration-parameters","title":"Model Configuration Parameters","text":""},{"location":"concepts/llm/#temperature","title":"Temperature","text":"<p>Temperature controls the creativity or randomness of the chatbot's responses.</p> <ul> <li>A low temperature (e.g., 0.1) makes the chatbot more deterministic, providing straightforward and predictable answers.</li> <li>A high temperature (e.g., 0.9) makes responses more creative, varied, or even surprising.</li> </ul>"},{"location":"concepts/llm/#example","title":"Example:","text":"<ul> <li>Low temperature: What's a dog? \u2192 A dog is a domesticated animal.</li> <li>High temperature: What's a dog? \u2192 A dog is a loyal companion, a furry friend who fills your life with wagging tails and boundless joy.</li> </ul> <p>The default temperature of 0.7 is a balanced choice designed to provide responses that are both varied and  interesting, while still being coherent.</p>"},{"location":"concepts/llm/#prompt","title":"Prompt","text":"<p>A prompt is the input or instructions given to the LLM to guide its response. It sets the context for the chatbot. Prompts can be as simple as a user question or as detailed as a conversation framework or role-play setup.</p>"},{"location":"concepts/llm/#example_1","title":"Example:","text":"<p>You are a helpful assistant. Answer questions clearly and concisely.</p>"},{"location":"concepts/llm/#tokens","title":"Tokens","text":"<p>Tokens are the building blocks of text that the LLM processes. A token might be a word, a part of a word, or even just punctuation.</p>"},{"location":"concepts/llm/#example_2","title":"Example:","text":"<p>The sentence \"Chatbots are cool.\" is broken into 4 tokens: <code>Chatbots | are | cool | .</code></p> <p>Tokens are important because they determine the cost and the processing complexity of an LLM's response.</p>"},{"location":"concepts/llm/#max-token-limit","title":"Max Token Limit","text":"<p>The max token limit is the maximum number of tokens the LLM can handle in a single interaction, including both the input (prompt) and output (response).</p>"},{"location":"concepts/llm/#example_3","title":"Example:","text":"<p>If the max token limit is 4096 tokens: - A long prompt with 2000 tokens leaves 2096 tokens available for the response.</p> <p>Understanding the token limit helps you create effective prompts without truncating responses.</p>"},{"location":"concepts/messaging_providers/","title":"Messaging Providers","text":"<p>Messaging providers offer access to communication platforms such as WhatsApp, Facebook Messenger, Slack, and more. Connecting a chatbot to these services allows users to interact with the bot on the respective service.```</p>"},{"location":"concepts/messaging_providers/#supported-providers","title":"Supported providers","text":"<p>Below is a list of supported providers and their integrated platforms in OCS:</p> <ul> <li>Twilio<ul> <li>WhatsApp</li> <li>Facebook Messenger</li> </ul> </li> <li>Turn.io<ul> <li>WhatsApp</li> </ul> </li> <li>Slack</li> <li>SureAhdere</li> </ul>"},{"location":"concepts/messaging_providers/#see-also","title":"See also","text":"<ul> <li>Configure a messaging provider</li> </ul>"},{"location":"concepts/participant_data/","title":"Participant Data","text":"<p>Participant data is the information that is collected from participants during their interactions with the system.</p> <p>Participant data is unique to each combination of channel platform, channel identifier and experiment. This means that the data for each bot may be different. For example, if the same person interacts with two different bots over the same channel e.g. WhatsApp, the data for each bot will be different. Furthermore, if the same person interacts with the same bot over two different channels e.g. WhatsApp and Telegram, the data for each channel will be different.</p> <p>The reason for this is to ensure that the data is only accessible to the bot that it is intended for and because there is no way for the system to know whether the same person is interacting with the bot on different channels.</p> <p>Participant data can be viewed and edited on the \"Participant Details\" page. This page can be accessed by clicking on the participant's name in the list of participants on the \"Participants\" page.</p>"},{"location":"concepts/participant_data/#using-participant-data","title":"Using participant data","text":""},{"location":"concepts/participant_data/#prompt-variable","title":"Prompt Variable","text":"<p>You can access the participant data using the <code>{participant_data}</code> prompt variable. This variable is a JSON object that contains the data for the participant. You can use this data to personalize the responses from the bot. For example, you can use the participant's name to personalize the greeting. For more information on prompt variables see here.</p> <p>Subsets of the data can be accessed using dot notation. For example, if you have a participant data object that looks like this:</p> <pre><code>{\n  \"name\": \"John Doe\",\n  \"address\": {\n    \"street\": \"123 Main St\"\n  },\n  \"tasks\": [\n    {\n      \"name\": \"Fix the roof\"\n    }\n  ]\n}\n</code></pre> <p>You can access specific parts of the data using the following prompt variables:</p> <pre><code>{participant_data.name}\n{participant_data.address.street}\n{participant_data.tasks[0].name}  # lists are zero-indexed\n</code></pre>"},{"location":"concepts/participant_data/#pipeline-nodes","title":"Pipeline Nodes","text":"<p>Other than using the prompt variable described above, there are also various pipeline nodes which allow you to access the participant data:</p> <ul> <li>Python node: This node allows you to access the participant data using Python code.</li> </ul>"},{"location":"concepts/participant_data/#system-properties","title":"System properties","text":"<p>There is only one system property that is set automatically by the system and only if the user interacts with a bot via the web channel. This is the <code>timezone</code> property. It is set to the timezone of the participant's browser. This is useful for localizing datetime variables in prompts.</p>"},{"location":"concepts/participant_data/#updating-participant-data","title":"Updating participant data","text":"<p>You can manually update the participant data using the Web UI. Participant data can also be updated dynamically using the methods described below.</p>"},{"location":"concepts/participant_data/#bot-tools","title":"Bot Tools","text":"<p>Open Chat Studio provides a tool that allows bots to update the participant data. This tool is available in the \"Tools\" tab of the experiment edit page.</p> <p>Making this tool available to a bot allows it to update the data in real time as the user is interacting with the bot.</p>"},{"location":"concepts/participant_data/#pipelines","title":"Pipelines","text":"<p>Both the \"Update Participant Data Node\" and the \"Python Node\" can be used to make updates to participant data. The \" Update participant data\" node is primarily used in conjunction with events (see below). The \"Python Node\" can be used to update the data using Python code as part of any pipeline.</p>"},{"location":"concepts/participant_data/#events","title":"Events","text":"<p>You can also update the participant data using events. This is useful if you want to update the data based on the context of the conversation. This method also allows you to specify the schema for the data that is being updated.</p> <p>An example of this is extracting tasks from the conversation history using a timeout event. The event could be configured to run 15 minutes after the last message was sent. The event would execute a pipeline which would extract the tasks from the conversation history and update the participant data using the appropriate pipeline node. In this example the following schema could be used:</p> <pre><code>{\n  \"tasks\": [\n    {\n      \"name\": \"name of the task\",\n      \"due_date\": \"due date of the task in the format YYYY-MM-DD\"\n    }\n  ]\n}\n</code></pre>"},{"location":"concepts/participant_data/#api","title":"API","text":"<p>You can also update the participant data using the API. This is useful if you want full control over the data or when you want to update the data based from an external system. You can use the following endpoint to update the participant data:</p> <p><code>POST /api/participants/</code></p> <pre><code>{\n  \"platform\": \"Name of the channel platform e.g. WhatsApp, Telegram etc.\",\n  \"identifier\": \"ID of the participant on the specified platform\",\n  \"name\": \"Optional name for the participant\",\n  \"data\": [\n    {\n      \"experiment\": \"ID of the experiment the data is for\",\n      \"data\": {\n        \"key\": \"value\"\n      }\n    }\n  ]\n}\n</code></pre> <p>See the API docs for more information on the API.</p>"},{"location":"concepts/prompt_variables/","title":"Prompt variables","text":"<p>Prompt variables are a great way to make your prompt dynamic or tailored to the participant by injecting data into specified placeholders. These variables are predefined and look like this:</p> <pre><code>{variable}\n</code></pre> <p>The following variables are currently supported:</p> <ul> <li><code>{source_material}</code> - The source material linked to your bot.</li> <li><code>{participant_data}</code> - Information specific to this participant, bot and channel. See here for more information. </li> <li><code>{current_datetime}</code> - This refers to the date and time at which the response is generated.</li> </ul> <p>Localizing injected datetime</p> <p>The injected datetime will be localized to the participant's timezone if it exists in their participant data. When a participant uses the web UI, their browser's timezone will automatically be saved to their participant data.</p> <p>A note on prompt caching</p> <p>Some LLM providers like OpenAI, use a technique called \"prompt caching\" to reduce latency and costs (See here). This happens automatically. However, caching is only effective for static data i.e. data that does not change. To take full advantage of this caching mechanism, you should place prompt variables near the end of your prompt whenever possible</p>"},{"location":"concepts/sessions/","title":"Chat Sessions","text":""},{"location":"concepts/sessions/#overview","title":"Overview","text":"<p>Chat sessions in Open Chat Studio define the scope of conversations between a user and a chatbot within a specific channel. Sessions are isolated, ensuring data privacy and contextual continuity for the duration of an interaction.</p>"},{"location":"concepts/sessions/#session-scope","title":"Session Scope","text":"<p>A session is uniquely defined by:</p> <ul> <li>User: The individual engaging with the chatbot.</li> <li>Channel: The platform through which the chat occurs (e.g., WhatsApp, Telegram, Web, Slack). See channels.</li> <li>Chatbot: The specific chatbot handling the conversation.</li> </ul> <p>Each session is independent, meaning:</p> <ul> <li>The session's data is bound to that session only and is not shared with other sessions.</li> <li>When a user interacts with a chatbot, the bot receives the session's history to maintain context.</li> <li>Multi-Session Channels: Channels such as Web, API, and Slack allow multiple active sessions per user, enabling parallel conversations.</li> <li>Single-Session Channels: Platforms like WhatsApp, Telegram, and SureAdhere support only one active session per user at a time.</li> </ul>"},{"location":"concepts/sessions/#history-management","title":"History Management","text":"<ul> <li>As conversations progress, all previous messages within a session are stored as <code>history</code>.</li> <li>If the session history exceeds a predefined maximum length, it is summarized, and the bot will only receive:</li> <li>A summary of older interactions.</li> <li>The most recent exchanges to maintain context.</li> </ul>"},{"location":"concepts/sessions/#participant-data","title":"Participant Data","text":"<p>Aside from session-specific data, Open Chat Studio maintains participant data, which:</p> <ul> <li>Persists across sessions.</li> <li>Is tied to the same <code>User, Channel, Chatbot</code> scope.</li> <li>Helps retain long-term user preferences and contextual information beyond a single session.</li> </ul>"},{"location":"concepts/sessions/#anonymous-sessions","title":"Anonymous Sessions","text":"<p>On the Web channel, users can have anonymous sessions, where:</p> <ul> <li>Participant data is only available for the duration of the session.</li> <li>Since user identity cannot be verified, data cannot persist beyond the session.</li> </ul>"},{"location":"concepts/sessions/#resetting-sessions","title":"Resetting Sessions","text":"<p>For Single-Session Channels like WhatsApp and Telegram, the current session continues indefinitely. However, sessions can be reset either manually by the user or automatically using Events or the API. When a session is reset:</p> <ul> <li>The current session is marked as completed.</li> <li>A new session is started with a fresh history.</li> </ul> <p>This means that, aside from participant data, the bot loses all information about the previous conversation \u2014 including the fact that it even took place.</p>"},{"location":"concepts/sessions/#manual-resets","title":"Manual resets","text":"<p>The chat user can manually reset the session (start a new session) by sending the <code>/reset</code> command. This command is available on all channels except Web and Slack.</p>"},{"location":"concepts/sessions/#automatic-resets","title":"Automatic resets","text":"<p>There are two ways to automatically reset a session:</p> <ul> <li>Events: You can configure an event to end the current session when the event is triggered. This will not automatically create a new session; however, if the user sends a message after the session is ended, a new session will be created. See Events.</li> <li>API: When using the Trigger Bot Message API, you can set <code>\"start_new_session\": true</code>, which will end the current session and start a new one before messaging the user.</li> </ul> <p>By structuring sessions in this way, Open Chat Studio ensures privacy-conscious, context-aware, and seamless interactions across different communication channels.</p>"},{"location":"concepts/source_material/","title":"Source material","text":"<p>Source Material is a feature that allows you to provide specific content, information, or data which the chatbot can use as a reference or knowledge base. This is particularly useful for LLM-based chatbots, as it helps tailor the chatbot\u2019s responses to be more aligned with your specific needs or the theme of the chatbot.</p>"},{"location":"concepts/source_material/#how-to-use-source-material","title":"How to use source material","text":"<ul> <li> <p>Content Integration: You can integrate specific documents or text into the chatbot. This could be information on a given health area in an FAQs format, program-specific data, or any other relevant content that you want your chatbot to reference. </p> </li> <li> <p>Contextual Relevance: By providing this material, you're essentially giving the chatbot a more focused and relevant context to operate within. This means your chatbot can provide more accurate and tailored responses based on the Source Materials you've provided.</p> </li> <li> <p>Updating Information: Keep your Source Materials up-to-date. As you update program materials (for example guidelines for counselling or home visits), updating your Source Materials will help ensure your chatbot remains relevant and effective. </p> </li> </ul>"},{"location":"concepts/source_material/#best-practices","title":"Best Practices","text":"<ul> <li> <p>Relevance and Accuracy: Ensure the materials are relevant to the conversations your chatbot will engage in. </p> </li> <li> <p>Organization and Structure: Well-organized Source Materials make it easier for the chatbot to retrieve and use the information. It's useful to structure your content in a clear, concise manner, with appropriate labels for different sections. </p> </li> </ul>"},{"location":"concepts/source_material/#see-also","title":"See also","text":"<ul> <li>Prompt variables</li> <li>Add a_knowledge base</li> </ul>"},{"location":"concepts/versioning/","title":"Versioning","text":"<p>Versioning is now enabled by default for all projects on Open Chat Studio. This comes with a few important changes that modify the default behavior of the platform.</p>"},{"location":"concepts/versioning/#terms","title":"Terms","text":"<p>OCS uses the following terms:</p> <ul> <li> <p>Unreleased Version. This is the version of the chatbot that exists when you click the edit button on the experiment. It can also be considered a \"draft\" or that it has \"unsaved changes\".</p> </li> <li> <p>Published Version. This is the version that users will interact with through the web, WhatsApp or any other configured channel--including the public link.</p> </li> </ul> <p>A note on version functionality</p> <p>Once a version is made, it cannot be edited or modified. This ensures that the users' experience remains stable even if the authors may be changing the unreleased version.</p> <p>Chatting to the unreleased version</p> <p>To chat to the unreleased version, navigate to the Experiment home page and click on the speech bubble icon at the top right corner of the page. There, a drop down will say either \"Unreleased Version\" or \"Published Version\". Select the Unreleased Version, and that will open a web chat. Only bot authors can chat with the unreleased version as it is not available through channels.This is a change in the default behavior of the platform as prior to versioning, all channels chatted to the unreleased version at all times.</p>"},{"location":"concepts/versioning/#changing-the-published-version","title":"Changing the Published Version","text":"<p>The published version can be selected from any released version of the experiment. To modify which version is the published version:</p> <ul> <li>Select \"View Details\" of the version</li> <li>Press the \"Set as Published Version\" button at the button of the dialog box.</li> </ul> <p>Alternatively, when a new version is being created, it can be set as the published version by marking the checkbox \"Set as Published Version\".</p> <p>Only one version can be the published version at a time.</p>"},{"location":"concepts/versioning/#workflow","title":"Workflow","text":"<p>When a new experiment is first created, there exists two versions, a published version and a unreleased version as shown in the version table: </p> <p>Then, when you would like to create another version after making changes to the unreleased version, you can either press the create version button on the table, or navigate to the edit experiment page and scroll to the bottom to locate the create version button. Note: this button will only be enabled if changes have been made to the version. </p> <p>That will take you the the create new version page which will show you the difference between the previous version (note not the published version) and the unreleased version. Here, you can also set this newly created version as the published version. Also, there is an option to add a description to the version that will be shown in the version table to quickly remember the changes between versions. </p> <p>Tada! There you have a new released version! You will be directed back the experiment verisons table where it may take a few minutes for the version to be fully available. Then you can chat with the version and view its details. When you select view details it shows the the detailed specifications of that version and if you navigate to the bottom, you are able to set as the published version and archive from that screen. </p> <p>If you click on the webchat button, for an unpublished version, there will be a banner indication that it's the unpublished version, and which version it is: </p> <p>For this demo, I released a few more versions for this experiment and also changed the published version. To easily see which is the published version for the experiment, look right of the experiment name at the top of the experiment home screen at the icon in green. For this example, you'll see \"v2\" which indicates that the version 2 is the published version. You will also be able to see in the table looking at the published version row for the checkmark. </p> <p>Versioning experiments that use OpenAI Assistants</p> <p>Can this be done? Yes! When an experiment is released that has an OpenAI Assistant, there is no additional configuration required. However, please note that a read-only copy of the OpenAI Assistant is made in Open Chat Studio (see in the Assistants tab) and also in OpenAI. This includes all reference files. The existing OpenAI Assistant prior to creating the the version will still be available and be able to be modified in the unreleased experiment version.</p> <p>Modifying Assistants in OpenAI referenced by released versions</p> <p>As mentioned above, the copied assistant will be read-only in OCS, however, in OpenAI changes can still be made to that copy of the assistant. We recommend advising your team to not modify this assistant if it references a released version. This can cause unexpected behavior to the version and to its end users. To ensure that the released version acts as expected this assistant should remain as-is.</p>"},{"location":"concepts/experiment/","title":"Experiments","text":"<p>An 'Experiment' is the current name used in Open Chat Studio to refer to a 'chatbot'. The name may change in the future.</p> <p>An Experiment links all the configuration and data for a chatbot including user sessions, data, actions etc.</p>"},{"location":"concepts/experiment/#experiment-types","title":"Experiment Types","text":"<p>There are three different types of chatbots that you can build in Open Chat Studio:</p> <ul> <li>Base language model</li> <li>Assistant</li> <li>Pipeline</li> </ul>"},{"location":"concepts/experiment/#base-language-model","title":"Base language model","text":"<p>This kind of bot is the most commonly used and simple to configure. It is backed the standard language model APIs such as the OpenAI chat completions API, Anthropic messages API or Google Gemini API.</p> <p>Bots configured in this way have all the basic features (memory, source material etc.) and can also use some of the advanced features like Scheduling and Reminders.</p>"},{"location":"concepts/experiment/#assistant","title":"Assistant","text":"<p>Assistant bots make use of OpenAI Assistants. The main advantage of using Assistants is that your bot gets access to the OpenAI tools:</p>"},{"location":"concepts/experiment/#code-interpreter","title":"Code Interpreter","text":"<p>This allows the bot to write and execute code to accomplish tasks.</p> <p>For more information see the OpenAI docs.</p>"},{"location":"concepts/experiment/#file-search","title":"File Search","text":"<p>This allows the bot to search and reference information provided in uploaded files. Unless your bot needs either of these capabilities, you should use a Base Language Model type bot.</p> <p>For more information see the OpenAI docs.</p>"},{"location":"concepts/experiment/#pipeline","title":"Pipeline","text":"<p>Pipelines allow you to create more complex bots by defining a \u2018graph\u2019 (in the computer science sense) of nodes. You can think of this graph as a workflow that flows from input to output. Each message to the bot is processed by the graph to produce a final output. A single response from the chatbot will be one successful path through the graph from the input node to the output node.</p> <p>This can be useful if you want to build a complex bot that performs different tasks depending on the user\u2019s request. Generally, trying to make a single bot prompt do multiple functions doesn\u2019t work well so it is better to create multiple prompts for each task and then combine them using a Pipeline. This is similar to the Multi-bot setup but allows more flexibility and complexity.</p>"},{"location":"concepts/pipelines/","title":"Pipelines","text":"<p>A pipeline is a way to build a bot by combining one or more steps together. </p> <p>Pipelines are the future</p> <p>In future, pipelines will be the default way to build bots in Open Chat Studio. They are a superset of existing functionality, enabling complex safety layers, routing and conditionals. There will be ample communication before we deprecate other bot building approaches. </p> <p>Here is an example of a very simple pipeline that uses an LLM to respond to the users input. This pipeline has a  single step that uses the LLM to generate a response.</p> A simple pipeline <p>Analyzing this pipeline from left to right:</p> <ul> <li>the user sends a message to the bot (this is the <code>input</code>)</li> <li>the message is then passed to the LLM which generates a response</li> <li>the response is then sent back to the user (this is the <code>output</code>)</li> </ul> <pre><code>graph LR\n  A@{ shape: stadium, label: \"Input\" } --&gt; B(LLM);\n  B --&gt; C@{ shape: stadium, label: \"Output\" };</code></pre> <p>Each time a user sends a message to the bot, the pipeline is executed and the final output is sent back to the user.</p> <p>Each 'step' in a pipeline is called a 'node' and pipelines can have multiple nodes. To learn more about the different types of nodes that can be used in a pipeline, see the node types documentation.</p>"},{"location":"concepts/pipelines/#advanced-example","title":"Advanced Example","text":"<p>Here is a more complex example that uses a LLM Router to route the input to one of three linked nodes. It also uses a Python Node to perform some custom logic.</p> <pre><code>graph TB\n  A@{ shape: stadium, label: \"Input\" } --&gt; Router(\"`**LLM Router**\n  Route to one of the linked nodes using an LLM`\");\n  Router --&gt;|GENERAL| Gen(LLM);\n  Router --&gt;|ROLEPLAY| Rp(LLM);\n  Router --&gt;|QUIZ| Py(\"`**Python Node**\n  Select a random quiz question`\");\n  Py --&gt; Qz(LLM);\n  Qz --&gt; Score(\"`**Python Node**\n  Update participant data with the score`\");\n  Gen --&gt; C@{ shape: stadium, label: \"Output\" };\n  Rp --&gt; C;\n  Score --&gt; C;  </code></pre>"},{"location":"concepts/pipelines/history/","title":"History Modes","text":"<p>There are several supported history modes for LLM-based nodes in a pipeline. Each is designed to solve a unique problem. In complex pipelines it is expected that a variety of history modes will be used across different nodes.</p> <p>Only valid for LLM nodes</p> <p>Note that <code>history</code> is only applicable to nodes that have an LLM response as they affect the conversational history sent to that LLM during inference, or completion. </p>"},{"location":"concepts/pipelines/history/#no-history","title":"No History","text":"<p>Nodes will default to <code>No History</code> as their history mode. This means that when a completion is requested from the LLM, no conversational history will be supplied. One common use case might be a formatting or translation node where the previous history may not be applicable to generating the correct output.</p>"},{"location":"concepts/pipelines/history/#node","title":"Node","text":"<p><code>Node</code> history will maintain a specific history for this particular node. The input to the node will be saved, along with the output from the LLM. </p> <p>LLM output is not necessarily the same as node output</p> <p>In a LLM Router node, the <code>output</code> from the node will be the same as the <code>input</code> to that node. That is, once it has done its routing, it will be a passthrough for the <code>input</code>. The output of the LLM however, will be the classification label. This is an important distinction to keep in mind.</p> <p>A common use case will be in a LLM Router node where we want to maintain a history of the node outputs (e.g., for continuity of what 'part' of the chatbot the user is interacting with), and we want to ensure that the history is using LLM outputs so that we don't unintentionally supply the LLM with few-shot examples of the wrong type of output.</p>"},{"location":"concepts/pipelines/history/#global","title":"Global","text":"<p>Nodes with <code>Global</code> history will supply the conversational history that the user would see to the LLM. The simple example uses a global history as the user is interacting directly with a single LLM. </p>"},{"location":"concepts/pipelines/history/#named","title":"Named","text":"<p>The final history mode is called <code>Named</code> and allows you to specify a specific, named, history that can be shared between nodes. Each node using the same shared history will contribute their <code>input</code> and LLM output to the history.</p> <p>Named history is updated immediately</p> <p>If there are multiple nodes serially that use the same <code>Named</code> history, then each node will add to the history. In the case of serial nodes, this will result in multiple new history entries for every processed user message.</p> <p>The most common use case to this will be when we have multiple parallel nodes after an LLM Router. In the Advanced Pipelines Example, the general, quiz, and roleplay LLM nodes would all likely use the same shared history, giving each node visibility into the larger conversation.</p> <p>Note that for this particular example, each of the nodes could use a <code>Global</code> history to achieve the same thing. However, if there was a translation or formatting node at before the final <code>output</code>, then the <code>Named</code> history mode would enable the interim nodes to share a history in the original language / formatting. </p>"},{"location":"concepts/pipelines/nodes/","title":"Node Types","text":""},{"location":"concepts/pipelines/nodes/#llm","title":"LLM","text":"<p>Uses an LLM to respond to the input.</p>"},{"location":"concepts/pipelines/nodes/#llm-router","title":"LLM Router","text":"<p>Routes the input to one of the linked nodes using an LLM. In this case, the LLM acts as a classifier using the prompt provided to classify an incoming message into a set of discrete categories that allow messages to be routed.</p> <p>Constrained outputs</p> <p>Currently the LLM Router node does not enforce constrained outputs, however, in the very near future, they will. This will be accomplished using a strict <code>json_mode</code> (for supporting models) that ensure that the LLM only generates one of the valid classification labels.</p> <p>The <code>outputs</code> listed by the node are the available classification labels. These should match the classification categories specified in your prompt. They can be adjusted through the <code>Advanced</code> settings for the node. The top output, which is prepended by a blue <code>*</code> is the default label. In the event that the LLM generates a response outside of the specified <code>outputs</code>, the route with the default label will be taken.</p> <p>Best practices for configuring a LLM Router</p> <p>It is advisable to use the Node history mode for an LLM Router to avoid unintentionally supplying few-shot examples to the node with an incorrect output format.</p>"},{"location":"concepts/pipelines/nodes/#static-router","title":"Static Router","text":"<p>Routes the input to a linked node using the participant data or temporary state of the pipeline.</p>"},{"location":"concepts/pipelines/nodes/#assistant","title":"Assistant","text":"<p>Uses an OpenAI assistant to respond to the input.</p>"},{"location":"concepts/pipelines/nodes/#python-node","title":"Python Node","text":"<p>The Python node allows the bot builder to execute custom Python code to perform logic, data processing, or other tasks.</p> <p>All the code must be encapsulated in a <code>main</code> function, which takes the node input as a string and returns a string to pass to the next node. The <code>main</code> function must also accept arbitrary keyword arguments to support future features. Here is an example of what the code might look like:</p> <pre><code>def main(input, **kwargs) -&gt; str:\n    # Put your code here\n    return input\n</code></pre> <p>The <code>input</code> parameter is a string that contains the input to the node. The return value of the function is a string that will be passed to the next node in the pipeline.</p> <p>The <code>kwargs</code> parameter is currently unused, but it is included to support future features that may require additional arguments to be passed to the function (though it is required to be present in the function signature).</p>"},{"location":"concepts/pipelines/nodes/#utility-functions","title":"Utility Functions","text":"<p>The Python node provides a set of utility functions that can be used to interact with the user's data and the pipeline state.</p>"},{"location":"concepts/pipelines/nodes/#python_node.get_participant_data","title":"<code>get_participant_data() -&gt; dict</code>","text":"<p>Returns the current participant's data as a dictionary.</p>"},{"location":"concepts/pipelines/nodes/#python_node.set_participant_data","title":"<code>set_participant_data(data: dict) -&gt; None</code>","text":"<p>Updates the current participant's data with the provided dictionary. This will overwrite any existing data.</p>"},{"location":"concepts/pipelines/nodes/#python_node.get_temp_state_key","title":"<code>get_temp_state_key(key_name: str) -&gt; str | None</code>","text":"<p>Returns the value of the temporary state key with the given name. If the key does not exist, it returns <code>None</code>.</p> <p>See also: Temporary State</p>"},{"location":"concepts/pipelines/nodes/#python_node.set_temp_state_key","title":"<code>set_temp_state_key(key_name: str, data: Any) -&gt; None</code>","text":"<p>Sets the value of the temporary state key with the given name to the provided data. This will override any existing data for the key.</p> <p>See also: Temporary State</p>"},{"location":"concepts/pipelines/nodes/#temporary-state","title":"Temporary State","text":"<p>The Python node can also access and modify the temporary state of the pipeline. The temporary state is a dictionary that is unique to each run of the pipeline (each new message from the user) and is not stored between sessions.</p> <p>The temporary state can be accessed and modified using the get_temp_state_key and set_temp_state_key utility functions.</p> <p>Temporary state contains the following keys by default. These keys can not be modified or deleted:</p> Key Description <code>user_input</code> The message sent by the user <code>outputs</code> The outputs generated by the previous node <code>attachments</code> A list of attachments passed in by the user. See Attachments <p>In addition to these keys, the temporary state can also contain custom key-value pairs that can be set and accessed by the Python node and by the Static Router node.</p> <p>Here is an example of a temporary state dictionary:</p> <pre><code>{\n    \"user_input\": \"Please help me with my problem\",\n    \"outputs\": {\n        \"Assistant\": \"I'm here to help! What can I do for you?\"\n    },\n    \"attachments\": [\n        Attachment(...),\n    ],\n    \"my_custom_key\": \"my_custom_value\",\n}\n</code></pre>"},{"location":"concepts/pipelines/nodes/#attachments","title":"Attachments","text":"<p>Part of the temporary state is a list of attachments. Attachments are files that the user has uploaded to the bot. Each attachment has the following fields:</p> Field Description <code>name</code> The name of the file <code>size</code> The size of the file in bytes <code>content_type</code> The MIME type of the file <code>upload_to_assistant</code> Whether the file should be uploaded to the assistant as an attachment <code>read_bytes()</code> Reads the attachment content as bytes. <code>read_text()</code> Reads the attachment content as text. <p>Here is an example of an attachment object:</p> <pre><code>attachment = Attachment(\n    name=\"proposal.pdf\",\n    size=1234,\n    content_type=\"application/pdf\",\n    upload_to_assistant=False,\n)\ncontent = attachment.read_text()\n</code></pre> <p>Attachment file types</p> <p>The Python node currently only supports reading the contents of the following file types:</p> <ul> <li>Text files (this includes files like CSV, JSON etc)</li> <li>PDF files</li> </ul> <p>Other file types can still be uploaded to assistants but the Python Node is not able to read the file contents using the <code>read_text()</code> method on the attachment.</p>"},{"location":"concepts/pipelines/nodes/#template","title":"Template","text":"<p>Renders a Jinja template.</p>"},{"location":"concepts/pipelines/nodes/#email","title":"Email","text":"<p>Send the input to the specified list of email addresses. This node acts as a passthrough, meaning the output will be identical to the input, allowing it to be used in a pipeline without affecting the conversation.</p>"},{"location":"concepts/pipelines/nodes/#extract-structured-data","title":"Extract Structured Data","text":"<p>Extract structured data from the input. This node acts as a passthrough, meaning the output will be identical to the input, allowing it to be used in a pipeline without affecting the conversation.</p>"},{"location":"concepts/pipelines/nodes/#update-participant-data","title":"Update Participant Data","text":"<p>Extract structured data and save it as participant data.</p>"},{"location":"concepts/team/","title":"Teams","text":"<p>Open Chat Studio is a multitenant platform that can support multiple organizations using the same instance at the same time. Each 'tenant' is called a 'team'. Teams are created by an organization and can have multiple members. Each team has its own settings and experiments.</p> <p>A user can be a member of multiple teams and have a different set of permissions in each team.</p> <p>A team serves as the root container for all data in Open Chat Studio.</p>"},{"location":"concepts/team/#team-configuration","title":"Team configuration","text":"<p>There is a set of global configuration that can be set at the team level. This includes:</p> <ul> <li>LLM Service Providers</li> <li>Speech Service Providers</li> <li>Messaging Providers</li> <li>Authentication Providers</li> <li>Custom Actions</li> <li>Tracing providers</li> </ul>"},{"location":"how-to/","title":"How-to Guides","text":"<p>Here you\u2019ll find answers to \u201cHow do I...?\u201d types of questions. These guides are goal-oriented and concrete; they're meant to help you complete a specific task. </p> <p>For conceptual explanations see the Conceptual guide.</p>"},{"location":"how-to/add_a_knowledge_base/","title":"Add a knowledge base","text":"<p>Adding knowledge to your bot depends on the type of bot you are building.</p>"},{"location":"how-to/add_a_knowledge_base/#base-llm-and-pipeline","title":"Base LLM and Pipeline","text":""},{"location":"how-to/add_a_knowledge_base/#add-your-source-material","title":"Add your source material","text":"<p>Select the Source Material tab on the left-hand menu and click Add new</p>"},{"location":"how-to/add_a_knowledge_base/#select-the-source-material-for-your-bot","title":"Select the source material for your bot","text":"<p>Once you\u2019ve created your source material, it should appear in the list of source materials when editing your bot.</p>"},{"location":"how-to/add_a_knowledge_base/#reference-the-source-material-in-your-prompt","title":"Reference the source material in your prompt","text":"<p>To reference the source material, include the <code>{source_material}</code> prompt variable in your prompt. Be mindful of its placement\u2014it\u2019s best to include it in a separate section rather than within a sentence.</p> <p>Example prompt:</p> <pre><code>You are a friendly bot. Be sure to reference the source material before answering the user's query: \n\n### Source material\n{source_material}\n</code></pre>"},{"location":"how-to/add_a_knowledge_base/#assistant","title":"Assistant","text":"<p>To add knowledge to your assistant, you must upload files to serve as the source material. When creating or editing your assistant, select the file_search or code_interpreter checkboxes to allow the assistant to read files.</p> <ul> <li>File search: This allows the bot to search and reference information provided in uploaded files.</li> <li>Code Interpreter: This allows the bot to write and execute code to accomplish tasks.</li> </ul>"},{"location":"how-to/add_a_knowledge_base/#see-also","title":"See also","text":"<ul> <li>Source Material</li> </ul>"},{"location":"how-to/api_access/","title":"API access","text":"<p>Using the API allows you to interact with your bot programmatically. This comes in especially useful when you're using a thrid party system to evaluate your bot. API access is enabled by default for all bots. To get started, you will need an API key, which you can generate by going to your profile page.</p> <p>See the API documentation for more details.</p>"},{"location":"how-to/configure_providers/","title":"Configure Providers","text":"<p>Providers are configured in your team settings. Before configuring a provider, ensure that you have an active account at the provider and access to the necessary integration credentials.</p>"},{"location":"how-to/deploy_to_different_channels/","title":"Deploy your bot to different platforms","text":"<p>To link a channel to your bot:</p> <p>Note</p> <p>Not all channels require a provider.</p> <ul> <li>Choose a provider for your channel.</li> <li>Configure a messaging provider. You will need to get the required credentials from your chosen provider.</li> <li>Once the provider is set up, navigate to your bot on OCS and click the plus icon in the \"Channels\" section.</li> <li>Choose your channel and complete the form. Follow the guide below to get the required information for each channel.</li> </ul>"},{"location":"how-to/deploy_to_different_channels/#web-and-api","title":"Web and API","text":"<p>The web channel uses the web interface and is enabled by default for all bots. Likewise, all bots can be accessed via the APIs.</p>"},{"location":"how-to/deploy_to_different_channels/#telegram","title":"Telegram","text":"<ul> <li>Follow this guide to create a Telegram bot.</li> <li>Copy the bot token and paste it into the form on OCS. It will look something like this: <code>4839574812:AAFD39kkdpWt3ywyRZergyOLMaJhac60qc</code>.</li> </ul> <p>Note</p> <p>Depending on your usecase, you probably want to disable group joins for your bot on Telegram. Since your telegram bot is public, anyone can add it to a group, which could end up costing you a lot. To achieve this, use the setjoingroups setting in BotFather.</p>"},{"location":"how-to/deploy_to_different_channels/#whatsapp","title":"WhatsApp","text":"<ul> <li>Add your WhatsApp number to the form.</li> <li>After you submit the form, you will be provided with a webhook URL. Copy this URL and paste it in the 'webhook url' input at your provider. You can also get this URL by clicking on your WhatsApp channel again.<ul> <li>For Twilio, see this page</li> <li>For Turn.io, go to your settings -&gt; API &amp; Webhooks -&gt; Add a webhook and paste the OCS webhook URL</li> </ul> </li> </ul>"},{"location":"how-to/deploy_to_different_channels/#facebook-messenger","title":"Facebook Messenger","text":"<p>Note</p> <p>It is assumed that you already have a Facebook page and a Twilio account with the Facebook page linked. Follow this guide if this is not the case.</p> <ul> <li>Add the ID of your Facebook page.</li> <li>After you submit the form, you will be provided with a webhook URL. Copy this URL and navigate back to your provider's settings to configure it with this URL.<ul> <li>For Twilio, edit your Facebook page settings and paste the URL into the \"Webhook URL for incoming messages\" field.</li> </ul> </li> </ul>"},{"location":"how-to/deploy_to_different_channels/#slack","title":"Slack","text":"<ul> <li>Choose the channel mode.</li> <li>If you chose to link a specific channel, enter the name of the Slack channel you want your bot to be available on.</li> <li>Once the channel is linked, you will be able to chat with it using the <code>@Dimagi Bots</code> reference.</li> </ul>"},{"location":"how-to/deploy_to_different_channels/#sureadhere","title":"SureAdhere","text":"<ul> <li>Enter the Tenant ID that would have been provided to you when setting up your SureAdhere account.</li> <li>After you submit the form, you will be provided with a webhook URL. Copy this URL and navigate back to your provider's settings to configure it with this URL.</li> </ul>"},{"location":"how-to/embed/","title":"Embedding a Bot","text":""},{"location":"how-to/embed/#overview","title":"Overview","text":"<p>Open Chat Studio provides two methods for embedding a chatbot into your website: using the Chat Component or an iframe. This guide covers both approaches.</p> <p>Here is a demo of using the chat component:</p> <p></p>"},{"location":"how-to/embed/#prerequisites","title":"Prerequisites","text":"<p>Before embedding, you must create a bot in Open Chat Studio.</p>"},{"location":"how-to/embed/#method-1-using-the-chat-component","title":"Method 1: Using the Chat Component","text":""},{"location":"how-to/embed/#installation-steps","title":"Installation Steps","text":"<ol> <li> <p>Add the widget script to your site's <code>&lt;head&gt;</code> section:</p> <pre><code>&lt;script type='module' src='https://unpkg.com/open-chat-studio-widget@0.3.0/dist/open-chat-studio-widget/open-chat-studio-widget.esm.js'&gt;&lt;/script&gt;\n</code></pre> </li> <li> <p>Obtaining Embed Code</p> <ol> <li>Log in to Open Chat Studio.</li> <li>Navigate to the Experiment you wish to embed.</li> <li>Click on the  Web channel and select  Share</li> <li>Copy the provided embed code snippet.</li> </ol> </li> <li> <p>Insert the widget where you want the chat button.</p> <p>The embed code snippet should look something like this:</p> <pre><code>&lt;open-chat-studio-widget \n  visible=\"false\" \n  bot-url=\"https://chatbots.dimagi.com/....\" \n  button-text=\"Let's Chat\"\n  position=\"right\"\n  expanded=\"false\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre> </li> </ol>"},{"location":"how-to/embed/#customization","title":"Customization","text":"<p>Customize the widget using CSS and CSS variables:</p> <pre><code>&lt;style&gt;\n  open-chat-studio-widget {\n    position: fixed;\n    right: 20px;\n    bottom: 20px;\n    --button-background-color: blue;\n    --button-background-color-hover: black;\n    --button-text-color: white;\n    --button-text-color-hover: yellow;\n  }\n&lt;/style&gt;\n&lt;open-chat-studio-widget \n  visible=\"false\" \n  bot-url=\"....\"\n  button-text=\"\ud83d\udc4b\"&gt;\n&lt;/open-chat-studio-widget&gt;\n</code></pre>"},{"location":"how-to/embed/#z-index","title":"Z-Index","text":"<p>If the chatbot appears below other elements on the page you can increase the <code>z-index</code> of the chatbot by setting the <code>--chat-z-index</code> CSS variable. The default value is <code>50</code>.</p> <pre><code>&lt;style&gt;\n  open-chat-studio-widget {\n    --chat-z-index: 1000;\n  }\n&lt;/style&gt;\n</code></pre> <p>In some cases it may also be necessary to reduce the z-index of other elements on the page.</p> <p>For more details, see Open Chat Studio Widget on npm</p>"},{"location":"how-to/embed/#method-2-embedding-via-iframe","title":"Method 2: Embedding via iframe","text":"<ol> <li> <p>Get Your Embed Code</p> <ol> <li>Log in to Open Chat Studio.</li> <li>Navigate to the Experiment you want to embed.</li> <li>Click on the  Web channel and select  Share</li> <li>Copy the provided embed code snippet.</li> </ol> </li> <li> <p>Add the Embed Code to Your Website</p> <p>If You Have a Website Builder (e.g., Wix, WordPress, Squarespace)</p> <ol> <li>Open your website editor.</li> <li>Find the option to add an HTML or Code Block.</li> <li>Paste the embed code into the block.</li> <li>Save and publish your changes.</li> </ol> <p>If You Have a Static HTML Website</p> <ol> <li>Open the HTML file of your website in a text editor.</li> <li>Locate the <code>&lt;body&gt;</code> section where you want the chat widget to appear.</li> <li>Paste the embed code just before the closing <code>&lt;/body&gt;</code> tag.</li> <li>Save the file and upload it to your web hosting service.</li> </ol> </li> </ol>"},{"location":"how-to/embed/#test-the-chatbot","title":"Test the Chatbot","text":"<ol> <li>Open your website in a web browser.</li> <li>Ensure the chatbot appears and functions as expected.</li> <li>Try sending a message to confirm it responds correctly.</li> </ol>"},{"location":"how-to/embed/#troubleshooting","title":"Troubleshooting","text":"<p>If the chatbot does not appear:</p> <ul> <li>Ensure you copied and pasted the embed code correctly.</li> <li>Clear your browser cache and refresh the page.</li> <li>Check that your website allows embedding external scripts.</li> </ul>"},{"location":"how-to/global_search/","title":"Using Global Search","text":"<p>Open Chat Studio has a global search feature that allows you to find objects using their public UUIDs. This is useful when you want to quickly find an object without navigating through the UI, especially if you have copied the UUID from a trace or another source.</p> <p>To use the global search feature, navigate to <code>https://chatbots.dimagi.com/search?q=UUID</code> where <code>UUID</code> is the public UUID of the object you want to find.</p> <p>If the object is found, and you have permissions to access it, you will be redirected to the page for that object.</p>"},{"location":"how-to/global_search/#objects-that-are-currently-supported","title":"Objects that are currently supported","text":"<ul> <li>Experiments</li> <li>ExperimentSessions</li> <li>Participants</li> </ul>"},{"location":"how-to/remote_api/","title":"Connecting to a remote API","text":"<p>Open Chat Studio allows you to connect to external services via HTTP API calls. This feature is useful for extending the functionality of your bot by integrating it with other services. This feature is analogous to OpenAI's GPT Actions feature.</p> <p>To do this you will need to create an action by navigating to the \"Custom Actions\" section in Team Settings. See the Custom Actions guide for more information on creating a Custom Action.</p>"},{"location":"how-to/remote_api/#using-the-custom-action-in-your-bot","title":"Using the custom action in your bot","text":"<p>Once you have created a custom action you can add it to your bot by following these steps:</p> <ol> <li>Open your Experiment's edit page.</li> <li>Navigate to the Tools tab.</li> <li>Select the action you want to add from the Custom Actions checkbox list.</li> </ol>"},{"location":"how-to/remote_api/#testing-the-custom-action","title":"Testing the custom action","text":"<p>To test the custom action, you can open a chat with your Experiment and type a message that triggers the action. The bot will make an HTTP request to the external service and return the response to you.</p> <p>To see more detail about the request and response, you can enable tracing in the Advanced tab of your Experiment.</p>"},{"location":"how-to/setting_up_a_survey/","title":"Setting up a survey","text":"<p>This page provides an overview of how to utilize surveys in your OCS chatbot.</p>"},{"location":"how-to/setting_up_a_survey/#what-are-surveys-on-open-chat-studio","title":"What are Surveys on Open Chat Studio?","text":"<p>The Surveys feature allows chatbot makers to give users a link to a Google form (or any other link to a survey), both at the start and end of an OCS chatbot web session.\u00a0</p> <p>External Channel Survey Limitations &amp; Workarounds</p> <p>Surveys will not be automatically presented to the user before or after the chat if you deploy your chatbot on external channels like WhatsApp or Telegram. If you would still like to capture pre- or post-survey questions using these channels, you can: </p> <ul> <li>Incorporate survey questions in your prompt and structuring the prompt such that the chatbot starts and ends with questions as you would like it to.\u00a0</li> <li>Send users links to a Google form or other kind of survey directly, before or after providing them with the link to the chatbot.\u00a0</li> </ul> <p>Example of a pre-survey when using an OCS bot on the web </p> <p>Example of a post-survey when using an OCS bot on the web </p>"},{"location":"how-to/setting_up_a_survey/#create-a-survey","title":"Create a Survey","text":"<p>The very first step is to create a survey and generate a web link for that survey. For example, you might use Google forms to create pre- and post-surveys. Once this step is complete, navigate to the \"Surveys\" option on the left-hand menu on Open Chat Studio and follow the steps given below to add your survey(s) to a chatbot.\u00a0</p>"},{"location":"how-to/setting_up_a_survey/#select-add-new","title":"Select \"Add New\".","text":"<ul> <li>Name: This is a name for you or your team members on OCS to identify different surveys.</li> <li>URL:\u00a0Add the URL of your survey.\u00a0</li> <li>Confirmation text:\u00a0This is the text a user sees when they see the link to the survey, before they begin to use the chatbot. You can edit this text as you'd like.\u00a0Here, it's also important to add <code>{survey_link}</code> where you would like to show the URL to your survey.</li> </ul> <p>Example</p> <p>Before starting the experiment, we ask that you complete a short survey. Please click on the survey link, fill it out, and, when you have finished, select the checkbox to confirm you have completed it. Survey link: {survey_link}.\u00a0</p> <p>If you would like to include both a pre-survey and a post-survey, repeat the above process for each survey.</p>"},{"location":"how-to/setting_up_a_survey/#final-step","title":"Final Step","text":"<p>Now edit your chatbot and choose which survey to use as the pre- or post survey.</p>"},{"location":"how-to/setting_up_a_survey/#using-google-forms","title":"Using Google Forms","text":"<p>Go to Google Forms and create your form. In order to link a particular participant, session and experiment with a specific form, you'll need to include questions with the titles \"Participant ID\", \"Session ID\" and \"Experiment ID\".</p> <p>For example: </p> <p>From here, click on the 3 dots in the top right corner and go to \"Get Prefilled Link\". Now fill in the fields that you want prefilled. In this case, \"Participant ID\", \"Session ID\" and \"Experiment ID\".</p> <p> When you click on \"Get Link\", you'll get a link that looks something like:</p> <p>https://docs.google.com/forms/some/uri/viewform?usp=pp_url&amp;entry.1118764343=participant&amp;entry.791635770=session&amp;entry.784126073=experiment</p> <p>Replace the sections in the URL as follows:</p> <ul> <li>participant -&gt; {participant_id}</li> <li>session -&gt; {session_id}</li> <li>experiment -&gt; {experiment_id}</li> </ul> <p>This will result in a link that looks like this:</p> <p>https://docs.google.com/forms/some/uri/viewform?usp=pp_url&amp;entry.1118764343={participant_id}&amp;entry.791635770={session_id}&amp;entry.784126073={experiment_id}</p> <p>This new link should be used for your survey link.</p>"}]}